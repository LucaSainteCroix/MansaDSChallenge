{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "from statistics import median, mean\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_rows\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month_windows_1 = pd.read_csv('data/df_month_windows_1.csv', parse_dates=[\"date\"])\n",
    "df_month_windows_3 = pd.read_csv('data/df_month_windows_3.csv', parse_dates=[\"date\"])\n",
    "df_week_windows_1 = pd.read_csv('data/df_week_windows_1.csv', parse_dates=[\"date\"])\n",
    "df_week_windows_6 = pd.read_csv('data/df_week_windows_6.csv', parse_dates=[\"date\"])\n",
    "# transactions['date'] = pd.to_datetime(transactions['date'])\n",
    "# users['update_date'] = pd.to_datetime(users['update_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2555, 27)\n",
      "(3441, 27)\n",
      "(8711, 47)\n",
      "(14757, 47)\n",
      "Index(['date', 'amount', 'amount_other', 'balance', 'account_id',\n",
      "       'business_NAF_code', 'user_id', 'month_sin', 'month_cos',\n",
      "       'previous_balance_1', 'previous_amount_1', 'previous_other_1',\n",
      "       'previous_balance_2', 'previous_amount_2', 'previous_other_2',\n",
      "       'previous_balance_3', 'previous_amount_3', 'previous_other_3',\n",
      "       'previous_balance_4', 'previous_amount_4', 'previous_other_4',\n",
      "       'previous_balance_5', 'previous_amount_5', 'previous_other_5',\n",
      "       'previous_balance_6', 'previous_amount_6', 'previous_other_6'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_month_windows_1.shape)\n",
    "print(df_month_windows_3.shape)\n",
    "print(df_week_windows_1.shape)\n",
    "print(df_week_windows_6.shape)\n",
    "print(df_month_windows_1.columns)\n",
    "final_dataframes = {\"month1\": df_month_windows_1, \"month3\": df_month_windows_3, \"week1\": df_week_windows_1, \"week6\": df_week_windows_6}\n",
    "# print(final_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Simple error \"\"\"\n",
    "    return actual - predicted\n",
    "\n",
    "def mae(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Mean Absolute Error \"\"\"\n",
    "    return np.mean(np.abs(_error(actual, predicted)))\n",
    "\n",
    "def mse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Mean Squared Error \"\"\"\n",
    "    return np.mean(np.square(_error(actual, predicted)))\n",
    "\n",
    "def rmse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Root Mean Squared Error \"\"\"\n",
    "    return np.sqrt(mse(actual, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(df, keywords):\n",
    "    to_remove = []\n",
    "    for col in df.columns:\n",
    "        for keyword in keywords:\n",
    "            if keyword in col:\n",
    "                to_remove.append(col)\n",
    "            \n",
    "    df = df.drop(to_remove, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the problem, we don't have much previous data to infer from, either 6 or 12 data points. It means that, if possible, we need to make use of all the information we have, that means\n",
    "\n",
    "Since I'm not proficient on this type of problem and data, and I want to learn more about it, I'm going to try out a lot of different things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I built some very basic models to compare our more advanced models to. I experimented with multiple combinations of features like adding the other_amount, the balance, one-hot-encoding the NAF code etc. The best results were with just previous amounts and balances.\n",
    "\n",
    "I chose Mean Absolute Error as a metric for its simplicity and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as mtr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.api import OLS\n",
    "from statsmodels.api import add_constant\n",
    "\n",
    "def run_basic_models(dataframes, features_to_remove):\n",
    "    results_dataframe = pd.DataFrame(columns=\n",
    "        [\"Naive\", \"Seasonal Naive\", \"Average\", \"Median\", \"Drift\", \"OLS_Regression\"],\n",
    "        index=dataframes.keys())\n",
    "    i=0\n",
    "    for name, df in dataframes.items():\n",
    "\n",
    "#         df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "        total_drifts_mae = []\n",
    "        ols_mae = []\n",
    "        naive_mae = []\n",
    "        average_mae = []\n",
    "        median_mae = []\n",
    "        naive_seasonal_mae = []\n",
    "        X = df.loc[:, df.columns != 'amount']\n",
    "        y = df[\"amount\"]\n",
    "        \n",
    "        X = remove_features(X, [\"other\"])\n",
    "        \n",
    "        kf = KFold(n_splits=5)\n",
    "\n",
    "        for train_index, test_index in kf.split(X,y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            X_train = X_train.drop([column for column in X_train.columns if column in features_to_remove], axis=1) # drop columns not in features\n",
    "            X_test = X_test.drop([column for column in X_test.columns if column in features_to_remove], axis=1) \n",
    "\n",
    "            drifts_mae = []\n",
    "            previous_amount_columns = [column for column in df.columns if \"previous_amount\" in column]\n",
    "            previous_amount_balance_columns = [column for column in df.columns if \"previous_amount\" in column or \"previous_balance\" in column]\n",
    "\n",
    "            \n",
    "            \n",
    "            # Naive Forecast (last value)\n",
    "            NaiveForecast = X_train['previous_amount_1']\n",
    "            naive_mae.append(mae(y_train, NaiveForecast))\n",
    "\n",
    "\n",
    "            # Average Forecast (average of all previous values)\n",
    "            average_forecast = X_train[previous_amount_columns].mean(axis=1)\n",
    "            average_mae.append(mae(y_train, average_forecast))\n",
    "\n",
    "            # Median Forecast (median of all previous values)\n",
    "            median_forecast = X_train[previous_amount_columns].median(axis=1)\n",
    "            median_mae.append(mae(y_train, median_forecast))\n",
    "\n",
    "            # Seasonal Naive Forecast (predict using value from 4 weeks ago for weeks, and just normal naive for month)\n",
    "            if \"week\" in name:\n",
    "                NaiveSeasonalForecast = X_train['previous_amount_4']\n",
    "                naive_seasonal_mae.append(mae(y_train, NaiveSeasonalForecast))\n",
    "            else:\n",
    "                NaiveSeasonalForecast = X_train['previous_amount_1']\n",
    "                naive_seasonal_mae.append(mae(y_train, NaiveSeasonalForecast))\n",
    "\n",
    "\n",
    "            # Drift Forecast (drawing a line from the first to the last observations and extrapolating from it)\n",
    "            row_count = 0\n",
    "            for idx, row in X_train.iterrows():\n",
    "                true_value = y_train.iloc[row_count]\n",
    "                y_t = row['previous_amount_1'] # last amount\n",
    "                m = (y_t - row[previous_amount_columns[-1]]) / len(previous_amount_columns)\n",
    "                h = np.linspace(0,len(previous_amount_columns)-1, len(previous_amount_columns))\n",
    "                drift = y_t + m * h\n",
    "                drifts_mae.append(mae(true_value, drift))\n",
    "                row_count+=1\n",
    "\n",
    "            total_drifts_mae.append(mean(drifts_mae))\n",
    "\n",
    "\n",
    "            # OLS Linear Regression (with previous values as regressors, so like an autoregression)\n",
    "#             X_train = add_constant(X_train)\n",
    "\n",
    "            model = OLS(y_train, X_train)\n",
    "            \n",
    "            model_fit = model.fit()\n",
    "            model_fit.save(name+\".pickle\")\n",
    "            X_test_ols =  X_test\n",
    "            ols_pred = model_fit.predict(X_test_ols)\n",
    "            median_X_test = X_test[previous_amount_columns].median(axis=1)\n",
    "            if \"month\" in name:\n",
    "                ols_pred = np.array([x if x > 0 else median_X_test.iloc[i] for i, x in enumerate(ols_pred)])\n",
    "            else:\n",
    "                ols_pred = np.array([x if x < 0 else median_X_test.iloc[i] for i, x in enumerate(ols_pred)])\n",
    "            ols_mae.append(mae(ols_pred, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        results_dataframe.iloc[i] = [mean(naive_mae), mean(naive_seasonal_mae), mean(average_mae), mean(median_mae), mean(total_drifts_mae), mean(ols_mae)]\n",
    "        i+=1\n",
    "\n",
    "\n",
    "    print(X_train.columns) # to see which features were used\n",
    "    print(results_dataframe)\n",
    "    return results_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['previous_balance_1', 'previous_amount_1', 'previous_balance_2',\n",
      "       'previous_amount_2', 'previous_balance_3', 'previous_amount_3',\n",
      "       'previous_balance_4', 'previous_amount_4', 'previous_balance_5',\n",
      "       'previous_amount_5', 'previous_balance_6', 'previous_amount_6',\n",
      "       'previous_balance_7', 'previous_amount_7', 'previous_balance_8',\n",
      "       'previous_amount_8', 'previous_balance_9', 'previous_amount_9',\n",
      "       'previous_balance_10', 'previous_amount_10', 'previous_balance_11',\n",
      "       'previous_amount_11', 'previous_balance_12', 'previous_amount_12'],\n",
      "      dtype='object')\n",
      "              Naive Seasonal Naive      Average       Median        Drift  \\\n",
      "month1  3374.712689    3374.712689  3039.817520  2982.363331  4510.578097   \n",
      "month3  3326.056145    3326.056145  3074.164159  3080.359561  4509.242378   \n",
      "week1   1181.301255    1075.108392   931.651462   881.639308  1591.123262   \n",
      "week6   1079.848293     983.399735   863.732649   811.757910  1468.490042   \n",
      "\n",
      "       OLS_Regression  \n",
      "month1    2874.089736  \n",
      "month3    2813.185690  \n",
      "week1      855.160620  \n",
      "week6      798.014036  \n"
     ]
    }
   ],
   "source": [
    "only_numerical_features = [\"id\", \"date\", \"user_id\", \"account_id\", \"business_NAF_code\", \"amount_other\", \"balance\"]\n",
    "only_prices = [\"id\", \"date\", \"user_id\", \"account_id\", \"business_NAF_code\", 'month_sin', 'month_cos', 'month_week_sin', 'month_week_cos', \"amount_other\", \"balance\"]\n",
    "\n",
    "r = run_basic_models(final_dataframes, only_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results look better when using less filtered datasets, but that's maybe just cause it's easier to be closer to the truth when the truth is often 0. The more filtered dataset contains less repetitive data, so maybe it could make sense to use it, especially because new data coming in will be last 12 weeks of a least 6 months history, so we're more less likely to have very few transactions.\n",
    "                \n",
    "Regression has the best results, let's try to beat that with more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to try a Random Forest Regressor, for its robustness to overfitting and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month results\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "{'max_depth': 7, 'n_estimators': 500}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   36.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2571.100233146333\n",
      "week results\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "{'max_depth': 7, 'n_estimators': 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  2.9min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816.0879575950358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# df_month_windows_1=df_month_windows_1.iloc[:100]\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def gridsearchRFR(df, param_grid):\n",
    "    df = pd.get_dummies(df, drop_first=True) # for categorical data like naf_code\n",
    "    account_ids = pd.Series(df[\"account_id\"].unique())\n",
    "    test_account_ids = account_ids.sample(frac=0.2, random_state=22)\n",
    "\n",
    "    df_test = df[df[\"account_id\"].isin(test_account_ids)]\n",
    "    df_train = df[~df[\"account_id\"].isin(test_account_ids)]\n",
    "\n",
    "\n",
    "    X_train = df_train.drop([\"date\", \"user_id\", \"account_id\", \"amount\", \"balance\", \"amount_other\"], axis=1)\n",
    "    y_train = df_train[\"amount\"]\n",
    "    X_test = df_test.drop([\"date\", \"user_id\", \"account_id\", \"amount\", \"balance\", \"amount_other\"], axis=1)\n",
    "    y_test = df_test[\"amount\"]\n",
    "\n",
    "\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid=param_grid,\n",
    "        cv=5, scoring='neg_mean_absolute_error', verbose=2, n_jobs=-1)\n",
    "\n",
    "    grid_result = gsc.fit(X_train, y_train)\n",
    "    best_params = grid_result.best_params_\n",
    "    print(best_params)\n",
    "\n",
    "    min_samples_split = best_params.get(\"min_samples_split\", 2)\n",
    "    min_samples_leaf = best_params.get(\"min_samples_leaf\", 1)\n",
    "    max_features = best_params.get(\"max_features\", \"auto\")\n",
    "    rfr = RandomForestRegressor(\n",
    "        max_depth=best_params[\"max_depth\"], \n",
    "        n_estimators=best_params[\"n_estimators\"],\n",
    "        min_samples_split = min_samples_split,\n",
    "        min_samples_leaf = min_samples_leaf,\n",
    "        max_features = max_features,\n",
    "        random_state=False, verbose=1)\n",
    "\n",
    "    rfr.fit(X_train, y_train)\n",
    "    predictions = rfr.predict(X_test)\n",
    "\n",
    "    mae_score = mae(predictions, y_test)\n",
    "    return mae_score\n",
    "\n",
    "param_grid = {\n",
    "            'max_depth': range(4,8),\n",
    "            'n_estimators': (100, 200, 500, 1000),\n",
    "        }\n",
    "print(\"month results\")\n",
    "mae_score_m3 = gridsearchRFR(df_month_windows_3, param_grid)\n",
    "print(mae_score_m3)\n",
    "\n",
    "print(\"week results\")\n",
    "mae_score_w6 = gridsearchRFR(df_week_windows_6, param_grid)\n",
    "print(mae_score_w6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is 2571 Mean Absolute Error. Better than the simple models above. The one-hot encoded naf_code adds around 80 features. One-hot encoding categorical variables with high cardinality can cause inefficiency in tree-based ensembles. Continuous variables will be given more importance than the dummy variables by the algorithm which will obscure the order of feature importance resulting in poorer performance.\n",
    "\n",
    "I chose to remove them, and to tune the model's hyperparameters with a cross-validated random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def randomsearchRFR(df):\n",
    "    account_ids = pd.Series(df_month_windows_3[\"account_id\"].unique())\n",
    "    test_account_ids = account_ids.sample(frac=0.2, random_state=22)\n",
    "\n",
    "    df_test = df_month_windows_3[df_month_windows_3[\"account_id\"].isin(test_account_ids)]\n",
    "    df_train = df_month_windows_3[~df_month_windows_3[\"account_id\"].isin(test_account_ids)]\n",
    "    \n",
    "    \n",
    "\n",
    "    X_train = df_train.drop([\"date\", \"user_id\", \"account_id\", \"amount\", \"balance\", \"amount_other\", \"business_NAF_code\"], axis=1)\n",
    "    y_train = df_train[\"amount\"]\n",
    "    X_test = df_test.drop([\"date\", \"user_id\", \"account_id\", \"amount\", \"balance\", \"amount_other\", \"business_NAF_code\"], axis=1)\n",
    "    y_test = df_test[\"amount\"]\n",
    "\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 40, stop = 300, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(50, 90, num = 10)]\n",
    "#     max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 4, 6, 8, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "\n",
    "    rsc = RandomizedSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_distributions =random_grid,\n",
    "        n_iter = 100,\n",
    "        random_state=22,\n",
    "        cv=5, scoring='neg_mean_absolute_error', verbose=2, n_jobs=-1)\n",
    "\n",
    "    rsc_result = rsc.fit(X_train, y_train)\n",
    "    best_params = rsc_result.best_params_\n",
    "\n",
    "    print(best_params)\n",
    "\n",
    "    rfr = RandomForestRegressor(\n",
    "        **best_params,\n",
    "        random_state=False, verbose=1)\n",
    "\n",
    "    rfr.fit(X_train, y_train)\n",
    "    predictions = rfr.predict(X_test)\n",
    "    mae_score = mae(predictions, y_test)\n",
    "    \n",
    "    return rsc_result, rfr, mae_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "{'n_estimators': 271, 'min_samples_split': 2, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_depth': 76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2164.8617681465385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 271 out of 271 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 271 out of 271 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "rsc_result, rs_rfr, mae_score_m3 = randomsearchRFR(df_month_windows_3)\n",
    "print(mae_score_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2979.24635306 -2974.57155658 -2962.22097241 -2953.51445007\n",
      " -2965.19141097 -2993.70393031 -2964.93680678 -2969.43343227\n",
      " -2955.1672958  -2985.87159894 -2996.97296996 -3003.94295584\n",
      " -2975.10163439 -3003.50320342 -2967.92920718 -3000.74352308\n",
      " -2960.72206283 -2999.29870994 -2969.86426744 -2996.00650246\n",
      " -2959.4072583  -2958.62599803 -3036.70453474 -2971.3043492\n",
      " -2968.71750405 -2970.89064315 -2984.5195747  -2971.8444927\n",
      " -2959.34429866 -2961.0661957  -2966.13759264 -2964.4120373\n",
      " -2967.31182314 -2961.59342761 -2989.02628828 -3028.72547064\n",
      " -2967.81047544 -3030.09952766 -2961.45998069 -2979.14243541\n",
      " -2964.93299614 -2969.61626225 -2969.9951     -2954.0606751\n",
      " -2955.94745643 -2967.68609487 -3002.00203447 -2985.64331292\n",
      " -2968.33444856 -2968.57286291 -2967.22102821 -2960.54014162\n",
      " -2982.35840864 -2987.47999571 -3035.32095169 -2977.42693988\n",
      " -3046.5455253  -2956.61179408 -2960.44096506 -2967.22959265\n",
      " -2969.24238121 -3004.77507383 -3013.32694135 -3076.10092853\n",
      " -3036.97872809 -2956.39720998 -2974.88257237 -3029.85863175\n",
      " -2998.83923356 -3001.60612069 -2973.07748547 -2966.56006666\n",
      " -2966.6438054  -2981.71514596 -2967.38205083 -2956.78486075\n",
      " -2988.23854576 -2963.86032253 -2987.15452093 -2986.08790858\n",
      " -2956.36968946 -2982.62588994 -2980.84971193 -2973.08693318\n",
      " -2969.96082995 -2970.753685   -2956.74898319 -2993.73036748\n",
      " -3042.70953721 -2971.15654302 -3040.51658672 -2966.15881371\n",
      " -2977.9603407  -2971.26113636 -2975.36041276 -2960.20250006\n",
      " -2988.82435511 -3016.97602218 -2966.51526427 -2963.6939384 ]\n"
     ]
    }
   ],
   "source": [
    "print(rsc_result.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the result is 2164, much better. But we can clearly see that changing the parameters resulted in virtually no improvement on the mean test score of all 5 cross validation. I was planning on doing grid search around the best random search parameters, but it seems irrelevant now. The Random Forest Regression is clearly underfitting and ineffective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just try training a new model with those parameters but with a different sampling random state so we don't train and evaluate on the same accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 271 out of 271 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2694.9355614583524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 271 out of 271 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# params = {'max_depth': 70, 'min_samples_leaf': 6, 'min_samples_split': 7, 'n_estimators': 300}\n",
    "params = {'n_estimators': 271, 'min_samples_split': 2, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_depth': 76}\n",
    "\n",
    "account_ids = pd.Series(df_month_windows_3[\"account_id\"].unique())\n",
    "test_account_ids = account_ids.sample(frac=0.2, random_state=21)\n",
    "\n",
    "df_test = df_month_windows_3[df_month_windows_3[\"account_id\"].isin(test_account_ids)]\n",
    "df_train = df_month_windows_3[~df_month_windows_3[\"account_id\"].isin(test_account_ids)]\n",
    "\n",
    "X_train = df_train.drop([\"date\", \"user_id\", \"account_id\", \"amount\", \"balance\", \"amount_other\", \"business_NAF_code\"], axis=1)\n",
    "y_train = df_train[\"amount\"]\n",
    "X_test = df_test.drop([\"date\", \"user_id\", \"account_id\", \"amount\", \"balance\", \"amount_other\", \"business_NAF_code\"], axis=1)\n",
    "y_test = df_test[\"amount\"]\n",
    "\n",
    "\n",
    "\n",
    "rfr = RandomForestRegressor(\n",
    "        **params,\n",
    "        random_state=False, verbose=1, n_jobs=-1)\n",
    "\n",
    "rfr.fit(X_train, y_train)\n",
    "predictions = rfr.predict(X_test)\n",
    "\n",
    "mae_score = mae(predictions, y_test)\n",
    "print(mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is much worse. Let's try with random sampling over the index and not the accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 271 out of 271 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 271 out of 271 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2716.92814881773\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 271, 'min_samples_split': 2, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_depth': 76}\n",
    "\n",
    "test_ids = df_month_windows_3.sample(frac=0.2, random_state=22).index\n",
    "\n",
    "df_test = df_month_windows_3.iloc[test_ids].sort_index()\n",
    "df_train = df_month_windows_3.loc[~df_month_windows_3.index.isin(test_ids)]\n",
    "\n",
    "\n",
    "X_train = df_train.drop([\"date\", \"user_id\", \"account_id\", \"amount\", \"balance\", \"amount_other\", \"business_NAF_code\"], axis=1)\n",
    "y_train = df_train[\"amount\"]\n",
    "X_test = df_test.drop([\"date\", \"user_id\", \"account_id\", \"amount\", \"balance\", \"amount_other\", \"business_NAF_code\"], axis=1)\n",
    "y_test = df_test[\"amount\"]\n",
    "\n",
    "\n",
    "\n",
    "rfr = RandomForestRegressor(\n",
    "        **params,\n",
    "        random_state=False, verbose=1, n_jobs=-1)\n",
    "\n",
    "rfr.fit(X_train, y_train)\n",
    "predictions = rfr.predict(X_test)\n",
    "\n",
    "mae_score = mae(predictions, y_test)\n",
    "print(mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, mediocre results. I could try to refine the hyperparameters even more by plotting each parameter's effect on the score but I want to try another approach that I think could be more effective.\n",
    "\n",
    "After doing more research on panel data and how to model it and forecast with it, I have learned about fixed effects, random effects, and mixed models.\n",
    "\n",
    "After reading everything, it seems like the most promising way to have a suitable model to forecast monthly incomes across multiple accounts. It models the relationships over time but also across groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month = pd.read_csv('data/df_monthly_data.csv', parse_dates=[\"date\"])\n",
    "df_week = pd.read_csv('data/df_weekly_data.csv', parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried modeling monthly income according to balance and monthly expense, grouped by account_id, then grouped by NAF_code, which showed better results. It doesn't allow to forecast but it showed good relationship between those variables grouped by naf code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "1\n",
      "[126, 201, 333]\n"
     ]
    }
   ],
   "source": [
    "# min and max number of month data for an account\n",
    "print(max(df_month.groupby(\"account_id\")['date'].count().values))\n",
    "print(min(df_month.groupby(\"account_id\")['date'].count().values))\n",
    "\n",
    "# let's delete accounts which have less than 7 months of data available\n",
    "idx_to_delete = []\n",
    "for idx, count in enumerate(df_month.groupby(\"account_id\")['date'].count()):\n",
    "    if count < 7:\n",
    "        idx_to_delete.append(idx)\n",
    "\n",
    "account_ids = df_month.groupby(\"account_id\")['date'].count().index\n",
    "\n",
    "account_ids_to_delete = [account_ids[id_to_delete] for id_to_delete in idx_to_delete]\n",
    "print(account_ids_to_delete)\n",
    "df_month = df_month.loc[~df_month[\"account_id\"].isin(account_ids_to_delete)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 9)\n"
     ]
    }
   ],
   "source": [
    "# make dates universal accross accounts (0 for first month of data and increment by 1 after)\n",
    "account_dfs=[]\n",
    "for _, df_account in df_month.groupby(\"account_id\"):\n",
    "    df_account = df_account.sort_values(by=\"date\", ascending=False)\n",
    "    df_account[\"date\"] = range(0, df_account.shape[0])\n",
    "    \n",
    "    account_dfs.append(df_account.sort_values(by=\"date\"))\n",
    "\n",
    "df_month = pd.concat(account_df for account_df in account_dfs)\n",
    "print(df_month.loc[df_month[\"amount\"]==0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2539.7481653927584\n",
      "2037.5380132287037\n",
      "1757.1153673261424\n",
      "1591.704749543777\n",
      "2133.8986144334885\n"
     ]
    }
   ],
   "source": [
    "from linearmodels.panel import PooledOLS\n",
    "import statsmodels.api as sm\n",
    "from linearmodels.panel import RandomEffects\n",
    "from linearmodels.panel import PanelOLS\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "\n",
    "pred_results = []\n",
    "models=[]\n",
    "\n",
    "for i in range(10, 15):\n",
    "    account_ids = pd.Series(df_month[\"account_id\"].unique())\n",
    "    test_account_ids = account_ids.sample(frac=0.2, random_state=i)\n",
    "\n",
    "    df_test = df_month[df_month[\"account_id\"].isin(test_account_ids)]\n",
    "    df_train = df_month[~df_month[\"account_id\"].isin(test_account_ids)]\n",
    "\n",
    "    \n",
    "    y_test = df_test[\"amount\"]\n",
    "    \n",
    "    model = sm.MixedLM.from_formula(\"amount ~ balance + amount_other\", df_train, groups=df_train[\"business_NAF_code\"])\n",
    "    \n",
    "    result = model.fit()\n",
    "    models.append(result)\n",
    "    predictions = result.predict(df_test)\n",
    "    pred_result = mae(predictions, y_test)\n",
    "    \n",
    "    print(pred_result)\n",
    "    pred_results.append(pred_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month[\"mean_balance\"] = df_month.groupby('account_id')['balance'].transform('mean')\n",
    "df_month[\"previous_amount\"] = df_month[\"amount\"].shift(1).fillna(method='bfill')\n",
    "df_month[\"previous_balance\"] = df_month[\"balance\"].shift(1).fillna(method='bfill')\n",
    "df_month[\"previous_amount_other\"] = df_month[\"amount_other\"].shift(1).fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3729.558906025532\n",
      "3638.284668221423\n",
      "3306.530458812592\n",
      "3225.356325702513\n",
      "3502.013671698812\n"
     ]
    }
   ],
   "source": [
    "pred_results = []\n",
    "models=[]\n",
    "\n",
    "for i in range(23, 28):\n",
    "    account_ids = pd.Series(df_month[\"account_id\"].unique())\n",
    "    test_account_ids = account_ids.sample(frac=0.2, random_state=i)\n",
    "\n",
    "    df_test = df_month[df_month[\"account_id\"].isin(test_account_ids)]\n",
    "    df_train = df_month[~df_month[\"account_id\"].isin(test_account_ids)]\n",
    "    \n",
    "    \n",
    "    y_test = df_test[\"amount\"]\n",
    "    \n",
    "    model = sm.MixedLM.from_formula(\"amount ~ previous_amount + previous_balance + previous_amount_other\", df_train, groups=df_train[\"business_NAF_code\"])\n",
    "    \n",
    "    result = model.fit()\n",
    "    models.append(result)\n",
    "    predictions = result.predict(df_test)\n",
    "    pred_result = mae(predictions, y_test)\n",
    "    \n",
    "    print(pred_result)\n",
    "    pred_results.append(pred_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model, as could be expected, performs poorly when only using previous month data. Let's use the 6 previous months data with our windows dataset, to see if a Mixed Linear Model could perform well and make forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2785.4572625055544\n",
      "3686.4419098096387\n",
      "3174.464343712196\n",
      "2978.794126172\n",
      "2982.023450924204\n"
     ]
    }
   ],
   "source": [
    "pred_results = []\n",
    "models=[]\n",
    "\n",
    "for i in range(1, 6):\n",
    "    account_ids = pd.Series(df_month_windows_3[\"account_id\"].unique())\n",
    "    test_account_ids = account_ids.sample(frac=0.2, random_state=i)\n",
    "\n",
    "    df_test = df_month_windows_3[df_month_windows_3[\"account_id\"].isin(test_account_ids)]\n",
    "    df_train = df_month_windows_3[~df_month_windows_3[\"account_id\"].isin(test_account_ids)]\n",
    "    \n",
    "    previous_amount_balance_columns = [column for column in df_train.columns if \"previous\" in column]\n",
    "\n",
    "    \n",
    "    X_train = sm.add_constant(df_train[previous_amount_balance_columns])\n",
    "    y_train = np.asarray(df_train[\"amount\"])\n",
    "    X_test = sm.add_constant(df_test[previous_amount_balance_columns])\n",
    "    y_test = df_test[\"amount\"]\n",
    "\n",
    "    model = sm.MixedLM(y_train, X_train, groups=df_train[\"account_id\"])\n",
    "    result = model.fit()\n",
    "    models.append(result)\n",
    "    predictions = result.predict(X_test)\n",
    "    pred_result = mae(predictions, y_test)\n",
    "    print(pred_result)\n",
    "    pred_results.append(pred_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Mixed Linear Model Regression Results\n",
      "===========================================================================\n",
      "Model:                  MixedLM      Dependent Variable:      y            \n",
      "No. Observations:       2664         Method:                  REML         \n",
      "No. Groups:             75           Scale:                   27863210.6569\n",
      "Min. group size:        2            Log-Likelihood:          -26621.4523  \n",
      "Max. group size:        416          Converged:               No           \n",
      "Mean group size:        35.5                                               \n",
      "---------------------------------------------------------------------------\n",
      "                     Coef.    Std.Err.    z    P>|z|    [0.025     0.975]  \n",
      "---------------------------------------------------------------------------\n",
      "const                525.658    140.645  3.737 0.000     249.998    801.317\n",
      "previous_balance_1    -1.782 275415.138 -0.000 1.000 -539805.533 539801.970\n",
      "previous_amount_1      1.796 275415.138  0.000 1.000 -539801.955 539805.547\n",
      "previous_other_1       1.231 276711.630  0.000 1.000 -542343.598 542346.060\n",
      "previous_balance_2    -1.125                                               \n",
      "previous_amount_2      2.905  76449.853  0.000 1.000 -149836.054 149841.865\n",
      "previous_other_2       2.708  76449.853  0.000 1.000 -149836.252 149841.667\n",
      "previous_balance_3     2.817  76449.853  0.000 1.000 -149836.142 149841.776\n",
      "previous_amount_3      0.156      0.028  5.629 0.000       0.101      0.210\n",
      "previous_other_3       0.091      0.041  2.241 0.025       0.011      0.172\n",
      "previous_balance_4     0.054      0.020  2.684 0.007       0.014      0.093\n",
      "previous_amount_4      0.077      0.030  2.619 0.009       0.019      0.135\n",
      "previous_other_4       0.132      0.042  3.146 0.002       0.050      0.214\n",
      "previous_balance_5    -0.041      0.019 -2.085 0.037      -0.079     -0.002\n",
      "previous_amount_5      0.121      0.030  4.092 0.000       0.063      0.179\n",
      "previous_other_5       0.024      0.042  0.558 0.577      -0.059      0.107\n",
      "previous_balance_6     0.017      0.014  1.184 0.236      -0.011      0.045\n",
      "previous_amount_6     -0.015      0.027 -0.570 0.569      -0.068      0.037\n",
      "previous_other_6      -0.092      0.040 -2.317 0.021      -0.170     -0.014\n",
      "Group Var          59589.783     28.468                                    \n",
      "===========================================================================\n",
      "\n",
      "                        Mixed Linear Model Regression Results\n",
      "======================================================================================\n",
      "Model:                     MixedLM          Dependent Variable:          y            \n",
      "No. Observations:          2577             Method:                      REML         \n",
      "No. Groups:                70               Scale:                       20849211.9179\n",
      "Min. group size:           2                Log-Likelihood:              inf          \n",
      "Max. group size:           288              Converged:                   Yes          \n",
      "Mean group size:           36.8                                                       \n",
      "--------------------------------------------------------------------------------------\n",
      "                   Coef.     Std.Err.      z    P>|z|      [0.025           0.975]    \n",
      "--------------------------------------------------------------------------------------\n",
      "const               0.000 7715072741.115  0.000 1.000 -15121264710.693 15121264710.693\n",
      "previous_balance_1 -0.152      55348.124 -0.000 1.000      -108480.481      108480.176\n",
      "previous_amount_1   0.183      55348.124  0.000 1.000      -108480.146      108480.512\n",
      "previous_other_1   -0.357      55348.124 -0.000 1.000      -108480.686      108479.972\n",
      "previous_balance_2  0.022                                                             \n",
      "previous_amount_2   0.079     370727.600  0.000 1.000      -726612.666      726612.823\n",
      "previous_other_2   -0.090     370727.600 -0.000 1.000      -726612.834      726612.654\n",
      "previous_balance_3  0.033     370727.600  0.000 1.000      -726612.711      726612.778\n",
      "previous_amount_3   0.086          0.027  3.166 0.002            0.033           0.140\n",
      "previous_other_3    0.032          0.040  0.803 0.422           -0.046           0.110\n",
      "previous_balance_4  0.003          0.023  0.114 0.909           -0.042           0.047\n",
      "previous_amount_4   0.086          0.029  2.969 0.003            0.029           0.142\n",
      "previous_other_4    0.159          0.041  3.888 0.000            0.079           0.240\n",
      "previous_balance_5 -0.024          0.022 -1.089 0.276           -0.067           0.019\n",
      "previous_amount_5   0.140          0.029  4.849 0.000            0.083           0.197\n",
      "previous_other_5    0.051          0.041  1.232 0.218           -0.030           0.131\n",
      "previous_balance_6  0.061          0.017  3.681 0.000            0.029           0.094\n",
      "previous_amount_6   0.070          0.025  2.818 0.005            0.021           0.120\n",
      "previous_other_6   -0.044          0.036 -1.213 0.225           -0.115           0.027\n",
      "Group Var           0.000                                                             \n",
      "======================================================================================\n",
      "\n",
      "              Mixed Linear Model Regression Results\n",
      "==================================================================\n",
      "Model:               MixedLM   Dependent Variable:   y            \n",
      "No. Observations:    2667      Method:               REML         \n",
      "No. Groups:          74        Scale:                27099671.4558\n",
      "Min. group size:     2         Log-Likelihood:       -26614.6465  \n",
      "Max. group size:     391       Converged:            Yes          \n",
      "Mean group size:     36.0                                         \n",
      "------------------------------------------------------------------\n",
      "                     Coef.   Std.Err.   z    P>|z|  [0.025  0.975]\n",
      "------------------------------------------------------------------\n",
      "const                618.536  140.740  4.395 0.000 342.691 894.381\n",
      "previous_balance_1    -1.760                                      \n",
      "previous_amount_1      1.744                                      \n",
      "previous_other_1       1.167                                      \n",
      "previous_balance_2    -2.708                                      \n",
      "previous_amount_2      4.413                                      \n",
      "previous_other_2       4.230                                      \n",
      "previous_balance_3     4.385                                      \n",
      "previous_amount_3      0.081    0.028  2.901 0.004   0.026   0.136\n",
      "previous_other_3       0.028    0.041  0.684 0.494  -0.052   0.108\n",
      "previous_balance_4     0.034    0.022  1.554 0.120  -0.009   0.077\n",
      "previous_amount_4      0.027    0.030  0.921 0.357  -0.031   0.086\n",
      "previous_other_4       0.152    0.042  3.589 0.000   0.069   0.235\n",
      "previous_balance_5    -0.057    0.021 -2.712 0.007  -0.098  -0.016\n",
      "previous_amount_5      0.163    0.030  5.355 0.000   0.103   0.222\n",
      "previous_other_5       0.073    0.043  1.682 0.093  -0.012   0.157\n",
      "previous_balance_6     0.017    0.016  1.083 0.279  -0.014   0.048\n",
      "previous_amount_6      0.005    0.027  0.180 0.857  -0.048   0.057\n",
      "previous_other_6      -0.088    0.039 -2.228 0.026  -0.165  -0.011\n",
      "Group Var          80755.657   29.860                             \n",
      "==================================================================\n",
      "\n",
      "                       Mixed Linear Model Regression Results\n",
      "====================================================================================\n",
      "Model:                     MixedLM         Dependent Variable:         y            \n",
      "No. Observations:          2792            Method:                     REML         \n",
      "No. Groups:                71              Scale:                      25739514.7271\n",
      "Min. group size:           4               Log-Likelihood:             inf          \n",
      "Max. group size:           349             Converged:                  Yes          \n",
      "Mean group size:           39.3                                                     \n",
      "------------------------------------------------------------------------------------\n",
      "                   Coef.     Std.Err.      z    P>|z|      [0.025         0.975]    \n",
      "------------------------------------------------------------------------------------\n",
      "const               0.000 4651997900.306  0.000 1.000 -9117748340.756 9117748340.756\n",
      "previous_balance_1 -0.168     185481.878 -0.000 1.000     -363537.968     363537.632\n",
      "previous_amount_1   0.180     185481.878  0.000 1.000     -363537.620     363537.980\n",
      "previous_other_1   -0.374     183774.517 -0.000 1.000     -360191.808     360191.060\n",
      "previous_balance_2  0.026     175761.890  0.000 1.000     -344486.948     344486.999\n",
      "previous_amount_2   0.108                                                           \n",
      "previous_other_2   -0.136                                                           \n",
      "previous_balance_3  0.055                                                           \n",
      "previous_amount_3   0.152          0.029  5.292 0.000           0.096          0.208\n",
      "previous_other_3    0.077          0.039  1.959 0.050          -0.000          0.155\n",
      "previous_balance_4  0.049          0.020  2.434 0.015           0.010          0.088\n",
      "previous_amount_4   0.059          0.029  1.988 0.047           0.001          0.116\n",
      "previous_other_4    0.137          0.040  3.399 0.001           0.058          0.217\n",
      "previous_balance_5 -0.031          0.020 -1.578 0.115          -0.069          0.007\n",
      "previous_amount_5   0.077          0.030  2.582 0.010           0.018          0.135\n",
      "previous_other_5   -0.005          0.041 -0.132 0.895          -0.085          0.075\n",
      "previous_balance_6  0.002          0.015  0.152 0.879          -0.026          0.031\n",
      "previous_amount_6  -0.015          0.027 -0.560 0.575          -0.067          0.037\n",
      "previous_other_6   -0.066          0.038 -1.722 0.085          -0.141          0.009\n",
      "Group Var           0.000                                                           \n",
      "====================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Mixed Linear Model Regression Results\n",
      "==================================================================\n",
      "Model:               MixedLM   Dependent Variable:   y            \n",
      "No. Observations:    2791      Method:               REML         \n",
      "No. Groups:          77        Scale:                27509552.7194\n",
      "Min. group size:     2         Log-Likelihood:       -27871.7716  \n",
      "Max. group size:     368       Converged:            No           \n",
      "Mean group size:     36.2                                         \n",
      "------------------------------------------------------------------\n",
      "                     Coef.   Std.Err.   z    P>|z|  [0.025  0.975]\n",
      "------------------------------------------------------------------\n",
      "const                547.471  131.421  4.166 0.000 289.890 805.051\n",
      "previous_balance_1     2.170                                      \n",
      "previous_amount_1     -2.133                                      \n",
      "previous_other_1      -2.655                                      \n",
      "previous_balance_2    -6.729                                      \n",
      "previous_amount_2      4.550                                      \n",
      "previous_other_2       4.346                                      \n",
      "previous_balance_3     4.482                                      \n",
      "previous_amount_3      0.139    0.027  5.186 0.000   0.086   0.191\n",
      "previous_other_3       0.046    0.039  1.177 0.239  -0.031   0.124\n",
      "previous_balance_4     0.036    0.020  1.860 0.063  -0.002   0.075\n",
      "previous_amount_4      0.077    0.029  2.709 0.007   0.021   0.133\n",
      "previous_other_4       0.115    0.041  2.827 0.005   0.035   0.195\n",
      "previous_balance_5    -0.036    0.019 -1.863 0.062  -0.073   0.002\n",
      "previous_amount_5      0.182    0.029  6.230 0.000   0.125   0.239\n",
      "previous_other_5       0.071    0.042  1.705 0.088  -0.011   0.152\n",
      "previous_balance_6     0.025    0.014  1.764 0.078  -0.003   0.053\n",
      "previous_amount_6      0.039    0.026  1.479 0.139  -0.013   0.091\n",
      "previous_other_6      -0.001    0.039 -0.024 0.981  -0.077   0.075\n",
      "Group Var          39461.860                                      \n",
      "==================================================================\n",
      "\n",
      "[2769.494257027512, 3602.706057120732, 3075.2637568751684, 2874.777645590925, 2971.2278558790413]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model.summary())\n",
    "\n",
    "print(pred_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845.2194032526189\n",
      "1258.0980514523353\n",
      "986.5145460552178\n",
      "704.1772454954811\n",
      "779.2934281032846\n"
     ]
    }
   ],
   "source": [
    "pred_results = []\n",
    "models=[]\n",
    "\n",
    "for i in range(1, 6):\n",
    "    account_ids = pd.Series(df_week_windows_6[\"account_id\"].unique())\n",
    "    test_account_ids = account_ids.sample(frac=0.2, random_state=i)\n",
    "\n",
    "    df_test = df_week_windows_6[df_week_windows_6[\"account_id\"].isin(test_account_ids)]\n",
    "    df_train = df_week_windows_6[~df_week_windows_6[\"account_id\"].isin(test_account_ids)]\n",
    "    \n",
    "    previous_amount_balance_columns = [column for column in df_train.columns if \"previous\" in column]\n",
    "\n",
    "    \n",
    "    X_train = sm.add_constant(df_train[previous_amount_balance_columns])\n",
    "    y_train = np.asarray(df_train[\"amount\"])\n",
    "    X_test = sm.add_constant(df_test[previous_amount_balance_columns])\n",
    "    y_test = df_test[\"amount\"]\n",
    "\n",
    "    model = sm.MixedLM(y_train, X_train, groups=df_train[\"account_id\"])\n",
    "    result = model.fit()\n",
    "    models.append(result)\n",
    "    predictions = result.predict(X_test)\n",
    "    pred_result = mae(predictions, y_test)\n",
    "    print(pred_result)\n",
    "    pred_results.append(pred_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are not bad but on average worse than OLS regression. This type of model is made to model the variance between groups and among groups, using the variable that change within groups (amount, balance) and the variables that change between groups (naf_code). But unlike the references I read online, our forecast variable and our predictor variables are correlated, and we have very little data (6 or 12 points). I don't know enough about this field to create something more advanced with fixed/random/mixed effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in the meantime I have read about Vector Auto-Regression (VAR). In the VAR framework, all variables are treated symmetrically. They are all modelled as if they all influence each other equally.\n",
    "\n",
    "To use VAR, we need our time series to be stationary, after some tries, 33% of data couldn't reach stationarity (when using only amount and other amount, 40% when using balance), even when trying many differencing (1 to 5). And even then, the lack of data forced me to have a maximum of 3 lags for VAR, meaning it could only forecast using data from 1-3 weeks ago, and sometimes 0. So obviously it was not usable. I think this would have been a really good option if we could use more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "non_stationary_ids = []\n",
    "all_used_lags = {}\n",
    "amount_columns = [column for column in df_week_windows_6.columns if \"previous_amount\" in column]\n",
    "other_columns = [column for column in df_week_windows_6.columns if \"previous_other\" in column]\n",
    "results = []\n",
    "# for variable in [\"amount\", \"other\"]:\n",
    "used_lags = {}\n",
    "variable_columns = [column for column in df_week_windows_6.columns if \"previous_\"+variable in column]\n",
    "for idx, data in df_week_windows_1.iterrows():\n",
    "    amount_data = data[amount_columns].iloc[::-1]\n",
    "    other_data = data[other_columns].iloc[::-1]\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        diff_amount_data = amount_data.diff(i).fillna(0)\n",
    "        diff_other_data = other_data.diff(i).fillna(0)\n",
    "\n",
    "        if diff_amount_data.shape[0]>0 and diff_other_data.shape[0]>0:\n",
    "            amount_p_value = sm.tsa.stattools.adfuller(diff_amount_data)[1]\n",
    "            other_p_value = sm.tsa.stattools.adfuller(diff_other_data)[1]\n",
    "\n",
    "            if amount_p_value < 0.05 and other_p_value < 0.05:\n",
    "                used_lag = i\n",
    "                used_lags[idx] = used_lag\n",
    "                \n",
    "                df_data = pd.DataFrame(list(zip(diff_amount_data, diff_other_data)), columns=[\"amount\", \"other_amount\"])\n",
    "\n",
    "                var = VAR(df_data)\n",
    "                lag_results = var.fit(maxlags=3, ic='aic')\n",
    "                lag_order = lag_results.k_ar\n",
    "    \n",
    "                if lag_order > 0:\n",
    "                    predictions = lag_results.forecast(df_data.values[-lag_order:], 1)\n",
    "                    result = mae(predictions[0][0], df_week_windows_6.iloc[idx][\"amount\"])\n",
    "                    results.append(result)\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                non_stationary_ids.append(idx)\n",
    "\n",
    "            \n",
    "\n",
    "all_used_lags[variable] = used_lags\n",
    "print(np.mean(results))\n",
    "print((len(set(non_stationary_ids))/df_week_windows_6.shape[0])*100, \"% of the data is non stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purely for the sake of experimenting, I tried some classic econometrics models that work with seasonality, on the weekly expenses data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1554.2764758538742\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "amount_columns = [column for column in df_week_windows_1.columns if \"previous_amount\" in column]\n",
    "y_test = []\n",
    "predictions = []\n",
    "for idx, row in df_week_windows_1.iterrows():\n",
    "    data = row[amount_columns].iloc[::-1].values\n",
    "    amount = row[\"amount\"]\n",
    "    # create class\n",
    "    model = ExponentialSmoothing(data, seasonal_periods = 4)\n",
    "    # fit model\n",
    "    model_fit = model.fit()\n",
    "    # make prediction\n",
    "    y_hat = model.predict(model_fit.params)\n",
    "    \n",
    "    y_test.append(amount)\n",
    "    predictions.append(y_hat)\n",
    "    \n",
    "print(mae(np.array(y_test), np.array(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "### Run Grid Search for SARIMAX parameters (this code will take a while to run)\n",
    "\n",
    "\n",
    "def sarimax_gridsearch(ts, pdq, pdqs, dates, maxiter=50, metric=\"aic\"):\n",
    "\n",
    "    # Run a grid search with pdq and seasonal pdq parameters and get the best AIC value\n",
    "    ans = []\n",
    "    for comb in pdq:\n",
    "        for combs in pdqs:\n",
    "            try:\n",
    "                mod = sm.tsa.statespace.SARIMAX(ts,\n",
    "                                                order=comb,\n",
    "                                                seasonal_order=combs,\n",
    "                                                enforce_stationarity=False,\n",
    "                                                enforce_invertibility=False,\n",
    "                                                dates=dates)\n",
    "\n",
    "                output = mod.fit(maxiter=maxiter)\n",
    "                \n",
    "                if metric == \"bic\":\n",
    "                    ans.append([comb, combs, output.bic])\n",
    "                elif metric == \"mae\":\n",
    "                    ans.append([comb, combs, output.mae])\n",
    "                else:\n",
    "                    ans.append([comb, combs, output.aic])\n",
    "\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "                continue\n",
    "            \n",
    "    # Find the parameters with minimal AIC value\n",
    "\n",
    "    ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'mae'])\n",
    "\n",
    "    # Sort and return top 5 combinations\n",
    "    ans_df = ans_df.sort_values(by=['mae'],ascending=True)[0:5]\n",
    "    \n",
    "    return ans_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1847.9700426609515\n",
      "2930.2787383240975\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from random import random\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "\n",
    "amount_columns = [column for column in df_week_windows_6.columns if \"previous_amount\" in column]\n",
    "y_test = []\n",
    "predictions_mae = []\n",
    "predictions_aic = []\n",
    "best_results_mae = []\n",
    "best_results_aic = []\n",
    "p = d = q = range(0, 3)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "pdqs = [(x[0], x[1], x[2], 4) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "\n",
    "df_sample = df_week_windows_1.sample(50, random_state = 42)\n",
    "\n",
    "for idx, row in df_sample.iterrows():\n",
    "    data = np.array(row[amount_columns].iloc[::-1].values).astype('float')\n",
    "    amount = row[\"amount\"]\n",
    "    y_test.append(amount)\n",
    "    dates = []\n",
    "    for i in range(1, 13):\n",
    "        i_date = row['date'] - pd.DateOffset(weeks=i)\n",
    "        dates.append(i_date)\n",
    "    \n",
    "    dates = list(reversed(dates))\n",
    "    results_mae = sarimax_gridsearch(data, pdq, pdqs, dates=dates, metric=\"bic\")\n",
    "\n",
    "    fit_pdq_mae = results_mae.iloc[0][\"pdq\"]\n",
    "    fit_pdqs_mae = results_mae.iloc[0][\"pdqs\"]\n",
    "    \n",
    "    best_results_mae.append(results_mae)\n",
    "#     print(idx)\n",
    "#     print(data)\n",
    "#     print(results_mae)\n",
    "    model_mae = SARIMAX(data, order=fit_pdq_mae, seasonal_order=fit_pdqs_mae, initialization='approximate_diffuse')\n",
    "    model_fit_mae = model_mae.fit(disp=False)\n",
    "\n",
    "    y_hat_mae = model_fit_mae.forecast(1)[0]\n",
    "#     print(y_hat)\n",
    "#     print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    predictions_mae.append(y_hat_mae)\n",
    "    \n",
    "    results_aic = sarimax_gridsearch(data, pdq, pdqs, dates=dates, metric=\"mae\")\n",
    "\n",
    "    fit_pdq_aic = results_aic.iloc[0][\"pdq\"]\n",
    "    fit_pdqs_aic = results_aic.iloc[0][\"pdqs\"]\n",
    "    \n",
    "    best_results_aic.append(results_aic)\n",
    "\n",
    "    model_aic = SARIMAX(data, order=fit_pdq_aic, seasonal_order=fit_pdqs_aic, initialization='approximate_diffuse')\n",
    "    model_fit_aic = model_aic.fit(disp=False)\n",
    "\n",
    "    y_hat_aic = model_fit_aic.forecast(1)[0]\n",
    "\n",
    "    \n",
    "    predictions_aic.append(y_hat_aic)\n",
    "    \n",
    "    \n",
    "print(mae(np.array(y_test), np.array(predictions_aic)))\n",
    "print(mae(np.array(y_test), np.array(predictions_mae)))\n",
    "\n",
    "# print(best_results_mae)\n",
    "# print(best_results_aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, I tried using Facebook's Prophet tool for forecasting. It is supposed to work well even with few data. It has support for specifying seasonality, but after trial and error I found that it performs better with nothing. (I also tried on the monthly data but it had poor results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[126, 333]\n"
     ]
    }
   ],
   "source": [
    "# delete accounts who have less than 13 weeks of data (12 for training and 1 to test)\n",
    "idx_to_delete = []\n",
    "for idx, count in enumerate(df_week.groupby(\"account_id\")['date'].count()):\n",
    "    if count < 13:\n",
    "        idx_to_delete.append(idx)\n",
    "\n",
    "\n",
    "account_ids = df_week.groupby(\"account_id\")['date'].count().index\n",
    "\n",
    "account_ids_to_delete = [account_ids[id_to_delete] for id_to_delete in idx_to_delete]\n",
    "print(account_ids_to_delete)\n",
    "df_week = df_week.loc[~df_week[\"account_id\"].isin(account_ids_to_delete)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 13 weeks of data per account\n",
    "account_dfs=[]\n",
    "for _, df_account in df_week.groupby(\"account_id\"):\n",
    "    df_account = df_account.sort_values(by=\"date\", ascending=True)\n",
    "#     df_account[\"date\"] = range(0, df_account.shape[0])\n",
    "    \n",
    "    account_dfs.append(df_account.sort_values(by=\"date\", ascending=False).iloc[:13])\n",
    "\n",
    "df_week = pd.concat(account_df for account_df in account_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "import logging, sys\n",
    "\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "predictions = []\n",
    "y_true = []\n",
    "for account_df in account_dfs:\n",
    "    account_df = account_df[[\"date\", \"amount\"]]\n",
    "    account_df.columns = [\"ds\", \"y\"]\n",
    "    \n",
    "#     print(\"ree\\n\")\n",
    "    x = account_df.iloc[1:].sort_values(by=\"ds\", ascending=True)\n",
    "    y = account_df.iloc[0][1]\n",
    "#     print(x)\n",
    "    m = Prophet()\n",
    "\n",
    "    m.fit(x)\n",
    "    future = m.make_future_dataframe(periods=1, freq='W')\n",
    "    forecast = m.predict(future)\n",
    "    prediction = forecast[[\"yhat\"]].iloc[-1][0]\n",
    "    \n",
    "    if prediction > 0:\n",
    "        prediction = np.median(x[\"y\"])\n",
    "        \n",
    "    predictions.append(prediction)\n",
    "    y_true.append(y)\n",
    "\n",
    "score = mae(np.array(predictions), np.array(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961.2123421299352\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad results, but still worse than our linear regression. I am sure that with well thought out feature engineering and a simple regression we could have had better results. I have learned a TON of things about time series modeling and forecasting. I am disappointed I could not get better results with them, but it seems that the goal of this exercise was to see the thought process, rather than the actual result.\n",
    "\n",
    "My mistake was that at first, I thought more complex techniques, that are able to model more exogenous variables and/or seasonality, would perform better. I probably should have realised that our low number of observations would make complex models too inefficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TimeSeriesTest-kernel",
   "language": "python",
   "name": "timeseriestest-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
